\chapter{Event Selection}\label{c:ES}

	This chapter is describes the selection criteria for data and simulated Monte-Carlo events, along with the specific calibrations and configurations used in the extraction and reconstruction of the objects making up the analysis. The event selections described here were chosen to target the typical \VBFHBB\, final state topology described in Section \ref{t:VBF}.

	\section{ATLAS Event Data}

	The raw data from the ATLAS detector is stored in a proprietary data format used by the ATLAS experiment, the Analysis Object Data (AOD) format. This is the output of the event reconstruction software, with each event having a corresponding discrete entry. For Run-2 of the LHC experiment, this was upgraded to the xAOD format, which is readable by ROOT \cite{ROOT}, a modular software framework managed by CERN and designed specifically for analysis of large datasets with complex statistical analysis, visualisation of data and storage. The xAOD format is a many leveled branching tree structure, with nodes of the tree grouping together related information from each event, and has an associated Event Data Model (EDM) to standardise classes, interfaces and types for representation of an event facilitating simple analysis \cite{xAOD}.

	Analyses typical make use of a derivation framework to refine the complete xAOD into a more selective Derived xAOD (DxAOD) which will normally only the relevant objects to a target analysis, and results in a smaller dataset that is much easier to manipulate, store and operate over. These derivations are produced using the ATLAS bulk data processing framework Athena \cite{athena}. The computation framework used for analysis of the xAOD data is the internally developed AnalysisBase suite of tools. The analysis presented in this dissertation uses AnalysisBase Release \texttt{2.4.31} and made use of the EventLoop package for event processing.

	This set of tools is used for both the real event data and the simulated Monte-Carlo data, with DxAODs of both datasets forming the core data for any ATLAS physics analysis. These datasets, following from the large output rate of the LHC, are extremely large, necessitating the use of parallelised computation to perform any statistically significant analysis. The computational framework developed at ATLAS is designed to perform concurrent computation, and processing, making use of the Worldwide LHC Computing Grid \cite{grid} to provide the necessary hardware capacity.

	\section{Datasets}

	The proton-proton collision data was recorded in 2016 at a centre-of-mass energy of $\sqrt{s}=13$TeV. In this dissertation, Data Period D was used owing to limited storage space on analysis computing facilities. For events from the ATLAS detector to be considered usable for analysis, there are certain quality criteria that need to be passed by the event. Events are subdivided into luminosity \textit{blocks}, which are marked as \textit{good} if there are no flaws in the data integrity or missing information from the detector readout. The events were marked as \textit{clean} if there were no errors reported for the tracker or calorimiter components of the detector, and only clean events were studied.

	Information on whether certain luminosity blocks are marked as clean are contained within a configuration \textit{Good Runs List} (GRL).This analysis used the all year 25ns Good Runs List (Table \ref{t:files}, Appendix \ref{a:config}), resulting in a data luminosity of $4.6312$fb$^{-1}$.

	\section{Monte-Carlo simulated events}

		 The simulated VBF sample (Table \ref{t:files}, Appendix \ref{a:config}) was produced during the MC15c production period. This sample was produced using the NLO generator \textsc{powheg} configured using the CTEQ6L1 \cite{CTEQ} set of PDFs and interfaced with \textsc{pythia8} tuned to AZNLO \cite{AZNLO}. The response of the ATLAS detector to the Monte-Carlo events was simulated using the \textcal{GEANT4} \cite{geant4, geant4atlas} simulation, which recreates a configurable model of the ATLAS detector, and performs the same calibrations and reconstructions on the Monte-Carlo simulated events as the physical detector performs on data.

		 In order to accurately compare the simulated events from the Monte-Carlo samples with the real event dataset, it is necessary to normalise the Monte-Carlo samples to the total luminosity of the dataset, based on the theoretical cross-section for the interaction. The Monte-Carlo simulation assigns a weight $w_i$ to each event simulated, which are summed to give the total number of events in the Monte-Carlo. Each bin of any histogram in the results produced from the simulated data is reweighted using a scaling factor $w_{MC}$, given by

		 \begin{equation}
		 w_{MC} = \frac{\sigma k L}{N}
		 \end{equation}

		 where $\sigma$ is the theoretical cross section, $L$ the integrated luminosity of the real dataset, $N$ the total number of simulated events ($\Sigma_N w_i$) and $k$ the Real $K$-Factor, which is a correction to the leading order cross section to reproduce the higher order calculation for the interaction. This reweighting of the Monte-Carlo datasets allows valid comparison of the Monte-Carlo simulated events with varying data sample sizes, as in the case of the reduced data luminosity in this analysis.


	\section{Jet Extraction}

		The analysis is based on the jet objects from the detector contained in the DxAOD, the reconstruction of which is covered in Section \ref{d:jetreco}. Both the offline jet objects and the online equivalents are retrieved, however the method by which the full collection of jets is assembled differs in either case. For offline jet objects, the DxAOD contains a complete set of jets for each reconstruction algorithm, which are each associated to the relevant jet \btag\, information. Offline jets were calibrated in line with the 20.7 recommendations (Table \ref{t:config}). In addition, recorded individual jets were required to have \pt$>45$GeV.

		Recovering the trigger-level jet objects from the xAOD is done by assembling distinct object collections from the data into a single jet collection. The jets that satisfied the trigger requirements are stored as \textit{split}-jets in the data. These \textit{split}-jet objects are those that will have \btag\ decisions associated with them. Any duplicate \textit{split}-jets in this collection, determined by pairing jets and removing those with $\Delta R$ spacings below a threshold value of 0.3, are removed and the \btag\, information stored in a separate xAOD container is associated with the \textit{split}-jets. Following this all HLT trigger jets are retrieved, which do not possess \btag\, information. The full set of HLT jets is compared to the \textit{split}-jets and any duplicates are removed, again using $\Delta R$ matching, from the HLT jet set to form the \textit{nonsplit}-jets. The combination of the \textit{split}-jets and \textit{nonsplit}-jets forms the complete jet collection for the trigger level event.

		This complete collection of \textit{split} and \textit{nonsplit}-jets was taken as the comprehensive full set of jets for an event. The lack of associated \btag\ information for the \textit{nonsplit}-jets meant only jets from the \textit{split}-jet list could be designated as \bjets.

		\subsection{\bjets}

		The details of \bjet\ identification are covered in Section \ref{det:btagging}. Offline $b$-jets were tagged using the \textit{MV2c10}-tagger configured using the January 2017 recommendations (Table \ref{t:config}) with two defined efficiency working points: \textit{tight}, with an overall efficiency of 70\% and \textit{loose} with 85\% tagging efficiency. Online $b$-jets were tagged using the \textit{MV2c20}-tagger as configured during the data taking, which made use of the March 2016 Recommendations (Table \ref{t:config}) with two identically defined \textit{tight} and \textit{loose} working points. The use of the older \btagging\ algorithm in the decision for the online jets was forced. The online trigger objects in the DxAOD only contain the results of the evaluated \btag, the quantities used during the calculation were previously discarded. As such, this analysis was required to use the MV2c20 algorithm for HLT jets.


	\section{\VBFHBB\, Analysis Strategy}


	\label{es:as}
		Candidate \VBFHBB\, events are selected by requiring two \textit{central} \bjets\, which form the Higgs candidate and two high \pt VBF-tagging jets. The analysis presented in this disseration follows the selection criteria outlined in the 2016 Search for \VBFHBB\ at ATLAS \cite{VBFHbb8tev}. Searches using \VBFHBB\, consider two exclusive analysis channels of interesting events: the \textit{four-central} channel, which requires all four jets to be contained within the central region $|\eta| < 2.8$, and the \textit{two-central} channel which requires two jets in the central region and at least one forward jet.
		The analysis presented in this dissertation focuses on the \textit{two-central} channel as this is a more standard VBF topology.

		For the \textit{two-central} channel, the event was required to pass the \texttt{HLT\_j80\_bmv2c2070\_split\_\-j60\_bmv2c2085\_split\_j45\_320eta490} trigger. This trigger requires a single L1 jet ROI of $E_\text{T} > 40$GeV and $|\eta| < 2.5$, a second central jet ROI with $E_\text{T} > 25$, and a forward jet ROI with $E_\text{T} > 20$GeV and $3.1 < |\eta| < 4.9$.
		At the HLT, one central jet \btagged\, at the \textit{tight} working point with \pt $>80$GeV, and a jet with \pt$>60$GeV tagged at the \textit{loose} working point were both required. Finally a HLT forward jet with $E_\text{t}>45$ between $3.2 < |\eta| < 4.9$ was needed.

		Once the trigger was passed, the event was required to contain one jet with \pt$>95$GeV which was \btagged\, at the \textit{tight} working point and one additional jet with \pt$>70$GeV that passed the \textit{loose} \btag\, working point. One forward jet with $3.2 < |\eta| < 4.4$ and $p_{\text{T}}>60$GeV was required along with a final VBF jet with \pt$>20$GeV and $|\eta| < 4.4$. Finally the \pt of the $b\bar{b}$ pair was required to exceed 160 GeV. This cut is to remove kinematic sculpting of the $M_{bb}$ distribution, which for absent or lower \ptbb cuts has a pronounced bump in the $200$-$300$GeV $M_{bb}$ region. This bump is a result of the correlation between \mbb and \ptbb. By requiring the \ptbb cut the \mbb distribution forms a regular falling distribution. The cuts as applied in the analysis code are summarised in Table \ref{tab:cuts}

		\begin{table}[h]
		\caption[\textit{Two-central} channel event cuts]{Cutflow for the \textit{two-central} \VBFHBB\. channel.}
		\label{tab:cuts}
		\medskip
		\centering
		\begin{tabularx}{\textwidth}{p{2.5cm} X}\toprule
			Cut & Description \\\midrule
			Good Runs List & Event required to pass GRL.\\
			Clean Events & Event required to be unaffected by detector issues.\\
			Trigger & Event required to have passed the \texttt{HLT\_j80\_bmv2c2070\_split\_\-j60\_bmv2c2085\_split\_j45\_320eta490} trigger.\\
			$\geq2$ \textit{loose} & Event was required to contain at least two \bjets\ with \pt$>70$GeV tagged at the \textit{loose} working point.\\
			$\geq2$ light-jets & Event was required to contain at least two non-\bjets\ with \pt$>20$GeV.\\
			\textit{Tight} \bjet & Event was required to contain one \bjet\ with \pt$>95$GeV tagged at the \textit{tight} working point.\\
			Forward jet requirement & Event was required to contain at one non-\bjet\ with \pt$>60$GeV and $3.2 < |\eta| < 4.4$.\\
			\ptbb$>160$GeV & The chosen $b\bar{b}$ pair was required to have a combined \pt$>160$GeV \\
			\bottomrule
		\end{tabularx}
	\end{table}

		The events were required to be clean events, unaffected by any small detector issues, and the jets were assigned to components of the \VBFHBB\, event as described in the following procedure. All pairs of jets that passed the \textit{loose} working point (where either of the jet pair passed the \textit{tight} working point) were considered; the pair with the highest \ptbb was selected as the Higgs candidate. An identical iterative procedure was carried out to assign the VBF pair, using jets not marked for consideration as the Higgs candidate. One of the VBF jet pair was required to satisfy the forward jet selection criterion, and the highest invariant mass pair was selected.

		These conditions were identical for both the Monte-Carlo simulation and data, with the exception of the trigger requirements which were not required for the simulated samples.

		In a full ATLAS analysis\cite{VBFHbb8tev}, the signal is extracted from the selected events using a Boosted Decision Tree trained to extract the \VBFHBB\, events over non-Higgs backgrounds. Time constraints in this analysis prohibited a full BDT analysis, but discussion of boosted decision trees and training is covered in Appendix \ref{a:bdt}.


\endinput
