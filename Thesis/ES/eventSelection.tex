\chapter{Event Selection}\label{c:ES}

	This chapter is dedicated to describing the selection criteria for real and simulated event data used in the analysis, along with the specific calibrations and configurations used in the extraction and reconstruction of the objects making up the analysis. The event selections described here were chosen to target the analysis towards the typical \VBFHBB\, final state described in Section \ref{t:VBF}.

	\section{ATLAS Event Data}

	The raw data from the ATLAS detector is stored in a proprietary data format used by the ATLAS experiment, the Analysis Object Data (AOD) format. This is the output of the event reconstruction software on an event by event basis. For Run-2 of the LHC experiment, this was upgraded to the xAOD format, which is readable by ROOT \cite{ROOT}, a modular software framework managed by CERN and designed specifically for analysis of large datasets with complex statistical analysis, visualisation of data and storage. The xAOD format is a many leveled branching tree structure, with nodes of the tree grouping together related information from each event, and has an associated Event Data Model (EDM) to standardise classes, interfaces and types for representation of an event to facilitate simple analysis \cite{xAOD}.

	Analyses typical make use of a derivation framework to refine the complete xAOD into a more selective Derived xAOD (DxAOD) which will normally contain only the relevant objects to a target analysis, and results in a smaller dataset that is much easier to manipulate, store and operate over. These derivations are produced using the ATLAS bulk data processing framework Athena \cite{athena}, and analysis make use of the internal AnalysisBase suite of tools to perform research studies. This analysis was conducting using the EventLoop package for event processing.

	This framework is used for both the real event data and the simulated Monte-Carlo data, with DxAODs of both datasets being used as the core data for any ATLAS physics analysis. These datasets, following from the large output rate of the LHC, are extremely large, necessitating the use of parallelised computation to perform any statistically significant analysis. The computational framework developed at ATLAS is designed to perform concurrent computation, and processing makes use of the Worldwide LHC Computing Grid \cite{grid} to perform the analysis.


	\section{Event weights}

		 In order to accurately compare the simulated events from the Monte-Carlo samples with the real event dataset, it is necessary to normalise the Monte-Carlo samples to the total luminosity of the dataset, based on the theoretical cross-section for the interaction. The Monte-Carlo simulation assigns a weight $w_i$ to each event simulated, which are summed to give the total number of events in the Monte-Carlo. Each bin in results produced from the simulated data is reweighted using a scaling factor:

		 \begin{equation}
		 w_{MC} = \frac{\sigma k L}{N}
		 \end{equation}

		 where $\sigma$ is the theoretical cross section, $L$ the integrated luminosity of the real dataset, $N$ the total number of simulated events ($\Sigma_N w_i$) and $k$ the Real $K$-Factor, which is a correction to the leading order cross section to reproduce the higher order calculation for the interaction.


	\section{Samples}
		 Real event data was taken from the 2016 13 TeV run, with Data Period D used owing to limited storage space on analysis computing facilities. The \texttt{HIGG5D3} derivation was used for the data and Monte-Carlo sample, a full list of tags and files is given in Appendix \ref{a:config}. This analysis used the all year 25ns Good Runs List (Table \ref{t:files}), resulting a a data luminosity of $4.6312$b$^{-1}$. The simulated VBF sample (Table \ref{t:files}) was produced during the MC15c production period. The sample was produced using the NLO generator \textsc{powheg} configured using the CTEQ6L1 \cite{CTEQ} set of PDFs and interfaced with \textsc{pythia8} tuned to AZNLO \cite{AZNLO}.

	\section{\VBFHBB\, Analysis Strategy}
		 Following from the description of the \VBFHBB\, events in Section \ref{t:VBF}, target events are selected by requiring two central \bjets which form the Higgs candidate and two high \pt VBF jets. Searches using \VBFHBB consider two exclusive analysis channels of interesting events:, the \textit{four-central} channel requires all four jets to be contained within the central region $|\eta| < 2.8$ and the \textit{two-central} channel requires two jets in the central region and one forward jet. In this study, the online trigger level jets could not be extracted for the specific trigger chains used previously for the \textit{four-central} channel, so analysis focuses on the \textit{two-central} channel.

		For the \textit{two-central} channel, the event was required to pass the \texttt{HLT\_j80\_bmv2c2070\_split\_\-j60\_bmv2c2085\_split\_j45\_320eta490} trigger. This trigger requires a single L1 jet ROI of $E_\text{T} > 40$GeV and $|\eta| < 2.5$. In addition, a second central jet ROI with $E_\text{T} > 25$ and a forward jet ROI with $E_\text{T} > 20$GeV and $3.1 < |\eta| < 4.9$ are both required.
		At the HLT level, one central jet \btagged\, at the \textit{tight} working point with \pt $>80$GeV, and a jet with \pt$>60$GeV tagged at the \textit{loose} working point. finally a HLT forward jet with $E_\text{t}>45$ between $3.2 < |\eta| < 4.9$.

		Once the trigger was passed, the event was required to contain one jet with \pt$>95$GeV which was \btagged at the \textit{tight} working point and one additional jet with \pt$>70$GeV that passed the \textit{loose} \btag working point. One forward jet with $3.2 < |\eta| < 4.4$ and \pt$>60$GeV was required along with a final VBF jet with \pt$>20$GeV and $|\eta| < 4.4$. Finally the \pt of the $bb$ pair was required to exceed 160 GeV. (This is to remove sculpting EXPLAINNNN)


NOTES:
what do we need in this sectionL:
	xAODs
	offline recovery
	online recover
	central channels
	triggers used






	\section{Jet Extraction}

		The analysis is based on the jet objects reconstructed from the detector contained in the DxAOD. Both the offline jet objects and the online equivalents are retrieved, however the method by which the full collection of jets is assembled differs between each case. For offline jet objects, the DxAOD contains a complete set of jets for each reconstruction algorithm, which are each associated to the relevant jet btagging information.

		In selecting the trigger level offline jets, firstly all \textit{split-jets} that pass the trigger are retrieved from the trigger chain. Any duplicates, determined through $\Delta R$ matching, are removed and the \btag information is associated with the jets. Following this all L1 trigger jets are retrieved, which do not possess \btag information. The full set of L1 jets is compared to the \textit{split-jets} and any duplicates are removed from the HLT jet set to form the \textit{nonsplit-jets}. The combination of the \textit{split-jets} and \textit{nonsplit-jets} forms the complete jet collection for the trigger level event.
		%As the \textit{split-jets} are required to have been \btagged, they are limited to a range of $|\eta| < 2.8$\todo{confirm btagging range} and the forward jet is by necessity in the \textit{nonsplit-jets}  #

	\section{Jet Assignment}

		For both online and offline studies the jet collections are processed to extract the four \VBFHBB jets. Firstly separate collections of jets that pass the \textit{loose} \btag working point and lower \bjet \pt cut and jets that do not pass the \btag but pass the lower VBF jet cut are assembled.


This section describes the selection criteria required for the events and reconstructed objects used in the analysis. These cuts and criteria are designed with the \VBFHBB event topology in mind, along with the limitations introduced by considering the available trigger chains as discussed in Section \ref{VBF:TriggerChains}. These cuts are applied in the \VBFHBB analysis and the direct object comparison covered in Chapter \ref{c:OP}.

\section{Events}

Data events were required to pass the all year 25ns Good Runs List\footnote{\detokenize{data16_13TeV.periodAllYear_DetStatus-v88-pro20-21_DQDefects-00-02-04_PHYS_StandardGRL_All_Good_25ns.xml}} \REF{GRL} and also be Clean \REF{Clean}.

\section{Offline Jets}

	Offline jet reconstruction was performed by the anti-$k_t$ algorithm (R=0.4) as discussed in Section \ref{t:jetReco}. Jets were calibrated in line with the 20.7 recommendations \REF{jets:calib}. When considering individual jets during the analysis, all jets were required to have a \pt $> 45$ GeV to be recorded.

\section{Online Jets}

	Online Jet reconstruction is a mystery. A full collection of online jets was recovered by extracting the split jets (Section \ref{det:split) and L1 Jets (Section \ref{det:trigger:L1}). The full set of online jets was considered as the recovered split jets combined with any L1 jets that did not match any split jet. When considering individual jets during the analysis, all jets were required to have a \pt $> 45$ GeV to be recorded. \todo{This needs to be understood}

\section{Offline \textit{b}-jets}

	The specifics of $b$-tagging are covered in Section \ref{det:btagging}. Offline $b$-jets were tagged using the \textit{MV2c10}-tagger\footnote{Jan 2017 Recommendations: \detokenize{2016-20_7-13TeV-MC15-CDI-2017-01-31_v1.root}} with two defined efficiency working points: \textit{Tight}, with an overall efficiency of 70\% and \textit{Loose} with 85\% tagging efficiency.

\section{Online \textit{b}-jets}

	 Online $b$-jets were tagged using the \textit{MV2c20}-tagger\footnote{Mar 2016 Recommendations: \detokenize{2016-Winter-13TeV-MC15-CDI-March10_v1.root}} with two defined efficiency working points: \textit{Tight}, with an overall efficiency of 70\% and \textit{Loose} with 85\% tagging efficiency.


\endinput
